{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyCon 2017 ETL Workshop\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "* Test Optimizations\n",
    "* References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import shutil\n",
    "import sys\n",
    "\n",
    "ROOT_FOLDER = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction\n",
    "\n",
    "NOTE: To see how SQLAlchemy behaves with Postgres, use the `--debug-sql` option when running the processor.\n",
    "\n",
    "## Joins\n",
    "\n",
    "The SQLAlchemy ORM uses lazy loading by default.  For most OLTP queries, this is appropriate as it will not join on associated tables until you access the related model (via the relationship property).\n",
    "\n",
    "However, for OLAP queries you need to be careful.  The `Submission` model in this workshop sample is related to both `User` and `Form`.  When transforming the response, it accesses the `user` relationship property which will then trigger a separate query.  If there were very few users who created lots of submissions, this would be fine because SQLAlchemy loaded caches model instances by default.  However, we would generally expect that a single user would only make a single submission during a processing interval.  Therefore, both users and submissions may be large.\n",
    "\n",
    "You can configure SQLAlchemy to always avoid the additional queries by forcing the query to join on the related tables via [eager loading](http://docs.sqlalchemy.org/en/rel_1_1/orm/loading_relationships.html?highlight=joinedload#joined-eager-loading).  This is available in our workshop via the `joined_load` boolean kwarg."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## memory profiling\n",
    "\n",
    "Running the naive processor seems to continually consume more and more memory:\n",
    "    \n",
    "```bash\n",
    "python main.py generate medium\n",
    "mprof run python main.py process medium naive-single\n",
    "```\n",
    "\n",
    "![naive_single_1](./images/naive_single_1.png)\n",
    "\n",
    "First we use `--debug-mem` to only do memory profiling on the processor.  We get something like this:\n",
    "\n",
    "```\n",
    "Line #    Mem usage    Increment   Line Contents\n",
    "================================================\n",
    "     1     51.9 MiB      0.0 MiB   def process(extractor, transformer, loader):\n",
    "     2                                 \"\"\"\n",
    "     3                                 Extract-Transform-Load process\n",
    "     4\n",
    "     5                                 :param extractor: partial extractor function\n",
    "     6                                 :param transformer: partial transformer function\n",
    "     7                                 :param loader:  partial loader function\n",
    "     8                                 \"\"\"\n",
    "     9     52.1 MiB      0.2 MiB       submissions_generator = extractor()\n",
    "    10\n",
    "    11     52.1 MiB      0.0 MiB       events_generator = transformer(submissions_generator)\n",
    "    12\n",
    "    13     84.2 MiB     32.1 MiB       loader(events_generator)\n",
    " ```\n",
    "    \n",
    "`loader` seems to have the highest incremental memory usage, so we dig further by decorating the `naive-loader`:\n",
    "\n",
    "```python\n",
    "from memory_profiler import profile\n",
    "\n",
    "@profile\n",
    "def naive_loader(session: sa_orm.Session, events):\n",
    "  ...\n",
    "```\n",
    "\n",
    "Results are now:\n",
    "\n",
    "```bash\n",
    "Line #    Mem usage    Increment   Line Contents\n",
    "================================================\n",
    "    21     52.4 MiB      0.0 MiB   @log_metrics\n",
    "    22                             @profile\n",
    "    23                             def naive_loader(session: sa_orm.Session, events):\n",
    "    24     52.4 MiB      0.0 MiB       num_events = 0\n",
    "    25     82.8 MiB     30.3 MiB       for event in events:\n",
    "    26     82.8 MiB      0.0 MiB           session.add(event)\n",
    "    27     82.8 MiB      0.0 MiB           num_events += 1\n",
    "    28     86.3 MiB      3.5 MiB       session.flush()\n",
    "    29     86.3 MiB      0.0 MiB       return num_events\n",
    "```\n",
    "\n",
    "Next we look at the events generator (which is the transformer).  Unfortunately, the `@profile` decorator in the `memory_profiler` library does not work with generators that use the `yield from` syntax due to [issue #42](https://github.com/fabianp/memory_profiler/issues/42).  Instead, you can display the memory usage before and after:\n",
    "\n",
    "```python\n",
    "from memory_profiler import memory_usage\n",
    "\n",
    "def transform_submissions(session, submissions, processed_on:datetime.datetime=None):\n",
    "  ...\n",
    "    mem_before = memory_usage()[0]\n",
    "\n",
    "    for submission in submissions:\n",
    "        yield from _transform_submission(get_node_path_map, submission, processed_on)\n",
    "        num_submissions += 1\n",
    "\n",
    "    mem_after = memory_usage()[0]\n",
    "    LOGGER.info('mem increment {} MB'.format((mem_after - mem_before)))\n",
    "  ...  \n",
    "```\n",
    "\n",
    "Output:\n",
    "```\n",
    "[INFO:app.etl.transformers]: mem increment 28.34375 MB\n",
    "```\n",
    "\n",
    "It seems that despite using a generator, the events are staying in memory.  By adding every event to the SQLAlchemy session, the default behavior is to keep a reference to it in memory.  This can be addressed by flush the single object immediately after adding it using `session.flush()` (see `load-single-flush-single` processor configuration).\n",
    "\n",
    "```python\n",
    "from memory_profiler import profile\n",
    "\n",
    "@log_metrics\n",
    "@profile\n",
    "def individual_flush_loader(session: sa_orm.Session, events):\n",
    "    ...\n",
    "```\n",
    "\n",
    "The loader's incremental memory usage becomes:\n",
    "\n",
    "```bash\n",
    "Line #    Mem usage    Increment   Line Contents\n",
    "================================================\n",
    "    31     52.5 MiB      0.0 MiB   @log_metrics\n",
    "    32                             @profile\n",
    "    33                             def individual_flush_loader(session: sa_orm.Session, events):\n",
    "    34     52.5 MiB      0.0 MiB       num_events = 0\n",
    "    35     57.3 MiB      4.8 MiB       for event in events:\n",
    "    36     57.3 MiB      0.0 MiB           session.add(event)\n",
    "    37     57.3 MiB      0.0 MiB           session.flush([event])\n",
    "    38     57.3 MiB      0.0 MiB           num_events += 1\n",
    "    39     57.3 MiB      0.0 MiB       return num_events\n",
    "```\n",
    "\n",
    "The overall memory usage becomes:\n",
    "\n",
    "![load-single-flush_single_1](./images/load-single-flush_single_1.png)\n",
    "\n",
    "However, it is important to note that the insertion is very slow.  This is because there is an `INSERT` statement for each individual event.  This will result in very poor I/O performance:\n",
    "\n",
    "```sql\n",
    "INSERT INTO clover_dwh.response_events (form_id, form_name, user_id, user_full_name, submission_id, submission_created, processed_on, schema_path, value, answer_type)\n",
    "VALUES (...) RETURNING clover_dwh.response_events.id\n",
    "\n",
    "INSERT INTO clover_dwh.response_events (form_id, form_name, user_id, user_full_name, submission_id, submission_created, processed_on, schema_path, value, answer_type)\n",
    "VALUES (...) RETURNING clover_dwh.response_events.id\n",
    ". . .\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timing profiling\n",
    "\n",
    "If you run the `load-single-flush-single` through the python `cProfile`, you can visualize it with `gprof2dot` to see the timing bottleneck.\n",
    "\n",
    "![naive_single_gprof_1.png](./images/naive_single_gprof_1.png)\n",
    "\n",
    "Specifically, the bottleneck will be the naive loader because SQLAlchemy emits a SQL `INSERT` statement for each tranformed event.\n",
    "\n",
    "![naive_single_gprof_2.png](./images/naive_single_gprof_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking\n",
    "\n",
    "We are encountering a class space-time tradeoff with this processor.  Doing individual INSERTs to conserve memory leads to the process taking too long (I/O timing bottleneck).  Loading everything into memory before insertion leads to too much memory being consumed (memory bottleneck).\n",
    "\n",
    "By using batching (a.k.a. chunking(, you can pick a balance along the spectrum between this spectrum.  \n",
    "* For extraction, SQLAlchemy provides the [Query.yield_per()](http://docs.sqlalchemy.org/en/rel_1_1/orm/query.html?highlight=yield_per#sqlalchemy.orm.query.Query.yield_per) method which leverages database cursors\n",
    "* For loading, the you can combine generators with the [more_itertools.chunked()](https://more-itertools.readthedocs.io/en/latest/api.html#more_itertools.chunked) method\n",
    "\n",
    "If you run with the `chunked-objects-with-join` processor, the total time improves:\n",
    "\n",
    "```\n",
    "[INFO:__main__]: Copying scenario 'medium_number' from template\n",
    "[INFO:app.etl.transformers]: Transformed 300 JSON submissions\n",
    "[INFO:app.etl.loaders]: Inserted 12150 response events into database\n",
    "[INFO:__main__]: Elapsed time (seconds): 3.325\n",
    "```\n",
    "\n",
    "The total memory usage will increase, but only by a limited amount _which you control_.  Therefore, the memory is still bounded:\n",
    "\n",
    "![chunked_overall_memory_1](./images/chunked_overall_memory_1.png)\n",
    "\n",
    "The timing is improved because we reduce the number of SQL `INSERT` statements.\n",
    "\n",
    "![chunked_timing_1](./images/chunked_timing_1.png)\n",
    "\n",
    "The rows to be inserted are batched.\n",
    "\n",
    "![chunked_timing_2](./images/chunked_timing_2.png)\n",
    "\n",
    "You can even verify this by look at the SQL log with `--debug-sql` option."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducing ORM load\n",
    "\n",
    "You can further optimize load insertions by reducing ORM usage.  SQLAlchemy provides the [bulk_insert_mappings](http://docs.sqlalchemy.org/en/rel_1_1/orm/session_api.html?highlight=bulk_insert_mappings#sqlalchemy.orm.session.Session.bulk_insert_mappings) method which takes dictionaries as input rather than model instances.  According to the SQLAlchemy documentation, this will reduce latency because there is no “history” or session state management features in use.\n",
    "\n",
    "The `chunked-mappings_with_join` processor configuration will allow you to load using `bulk_insert_mappings`.  The memory usage is the same as `large-chunks-with-join` as it uses the same chunking parameters if you run it with `mprof run`.  The timing profile looks similar as well as inserts are chunked.\n",
    "\n",
    "![chunked_insert_mappings_timing_1](./images/chunked_insert_mappings_timing_1.png)\n",
    "\n",
    "However, the _total elapsed time_ as recorded in the default log output for `chunked_mappings_with_join` is noticeably faster than `large-chunks-with-join`.  This is even more apparent if you use a larger number of submissions (e.g. the `large_number_fewer_users` or `large_number_many_users` scenarios)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unit Test Optimizations\n",
    "\n",
    "Development turnaround can be shortened by optimizing repeated regression suite runs. In particular, You can avoid recreating a 'clean database' with every individual test case.\n",
    "\n",
    "For most Always run all your SQLAlchemy tests inside a transaction:\n",
    "* Provide a session fixture (SQLAlchemy `Session` instance) which is always rolled back.\n",
    "* Using `.flush()` rather than `.commit()` to \"persist\" write operations.\n",
    "\n",
    "This speeds up individual tests.  However, using `.flush()` should not be employed if your test actually needs to verify transaction rollback behavior.\n",
    "\n",
    "One of the slowest steps is the test database setup.  By introducing a `--keepdb` option to our `pytest` suite, we can force a teamplate test database to be reused by:\n",
    "* Setting the `base_dir` kwarg to a fixed path in the `testing.postgresql.Postgresql()` constructor.  \n",
    "* Preventing re-initialization of the Postgres extensions, schemas and tables if the test database did not previously exists\n",
    "\n",
    "This greatly reduces fixture setup time and allows us to immediately start running tests.  However, it is the responsibility of the developer to remove the test database if the model schema changes or a test accidentally persists a change to the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 2.66 s per loop\n"
     ]
    }
   ],
   "source": [
    "# no optimizations\n",
    "%timeit -n1 -r3 subprocess.call('pytest tests/unit/test_models.py', shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 1.15 s per loop\n"
     ]
    }
   ],
   "source": [
    "# with optimizations\n",
    "import os\n",
    "from tests.conftest import KEEPDB_PATH\n",
    "\n",
    "# ensure the first run creates the retained folder\n",
    "testdb_path = os.path.join(ROOT_FOLDER, KEEPDB_PATH)\n",
    "if os.path.exists(testdb_path):\n",
    "    shutil.rmtree(testdb_path)\n",
    "\n",
    "%timeit -n1 -r3 subprocess.call('pytest --keepdb tests/unit/test_models.py', shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* [pytest](https://docs.pytest.org/en/latest/contents.html)\n",
    "* Python profiling\n",
    "    * Timing\n",
    "        * [Profiling modules](https://docs.python.org/3/library/profile.html)\n",
    "        * [gprof2dot](https://github.com/jrfonseca/gprof2dot)\n",
    "    * Memory\n",
    "        * [memory_profiler](https://github.com/fabianp/memory_profiler)\n",
    "* [testing.postgres](https://github.com/tk0miya/testing.postgresql)\n",
    "* [Jupyter Notebook](http://jupyter.org/)\n",
    "    * [Cell Magics](https://ipython.org/ipython-doc/3/interactive/magics.html#cell-magics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
